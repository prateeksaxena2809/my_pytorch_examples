{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe3a6ae",
   "metadata": {},
   "source": [
    "## Download the necessary libraries\n",
    "- Numpy: for array creation, inv, transpose and matrix multiplication\n",
    "- math: here for exp(used in sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56fe0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218cae04",
   "metadata": {},
   "source": [
    "**Question 1:** Calculate Covariance Matrix\n",
    "\n",
    "Given a matrix where each vector represents a feature i.e. same feature value for different data points, calculate the covariance matrix.\n",
    "\n",
    "A covariance matrix is a feature * feature matrix to represent how separate features interact with each other.\n",
    "\n",
    "\n",
    "\n",
    "Steps:\n",
    "- subtract feature means from values.\n",
    "- for each pair of features\n",
    "    - calculate the elementwise product.\n",
    "    - sum all the elementwise product.\n",
    "    - divide by number of data points.\n",
    "    \n",
    "**NOTE:** I will write two solutions for this problem, using numpy and without using numpy to provide a bridge to understand parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "125f67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using numpy\n",
    "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n",
    "    means=[sum(feature_vector)/len(feature_vector) for feature_vector in vectors]\n",
    "    vectors=[[feature_value-means[i] for feature_value in feature_vector] for i,feature_vector in enumerate(vectors)]\n",
    "    covariance_matrix=[]\n",
    "    for i,vec1 in enumerate(vectors):\n",
    "        covariance_matrix.append([sum([vec1[k]*val for k,val in enumerate(vec2)])/(len(vectors[0])-1) for j,vec2 in enumerate(vectors)])\n",
    "    return covariance_matrix\n",
    "\n",
    "# using numpy\n",
    "def calculate_covariance_matrix_numpy(vectors: list[list[float]]) -> list[list[float]]:\n",
    "    vectors=np.array(vectors)\n",
    "    features,data_points=vectors.shape\n",
    "    means=np.mean(vectors,axis=1).reshape(-1,1)\n",
    "    vectors=vectors-means\n",
    "    covariance_matrix=vectors @ vectors.T\n",
    "    covariance_matrix/=(data_points-1)\n",
    "    return covariance_matrix.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7265bbe",
   "metadata": {},
   "source": [
    "**Question 2:** Solve linear equations using Jacobi algorithm\n",
    "\n",
    "Jacobi Algorithm is an iterative algorithm to solve linear equations\n",
    "Given a square matrix A(n equations, n unknowns) and a vector b, iteratively change an initialized vector x, so that\n",
    "\n",
    "$Ax=b$\n",
    "\n",
    "Algorithm\n",
    "1. Initialize x\n",
    "2. for each of n iterations update\n",
    "     \n",
    "     $x_i$ = (1/$A_i$$_i$) * ( $b_i$ - sum($A_i$$_j$ * $x_j$))\n",
    "\n",
    "**NOTE:**\n",
    "1. $b_i$ - sum($A_i$$_j$ * $x_j$) is basically b-Ax\n",
    "2. Although the method states rounding at every iteration, all test cases are only accepted if rounding is done at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n",
    "    diag_a=np.diag(A)\n",
    "    non_diag_a=A-np.diag(diag_a)\n",
    "    x=np.zeros(len(b))\n",
    "    x_temp=np.zeros(len(b))\n",
    "    for _ in range(n):\n",
    "        x_temp = (b - (non_diag_a @ x)) / diag_a\n",
    "        x=x_temp.copy()\n",
    "    return np.round(x,4).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e578ec",
   "metadata": {},
   "source": [
    "**Question 3:** K-Means Clustering\n",
    "\n",
    "Implement the K-Means algorithm for clustering given a set of points and a set of initial centroids. Run for a given number of iterations or until the clusters converge, whichever is faster.\n",
    "\n",
    "Steps:\n",
    "1. for each iteration\n",
    "    - assign each point to a centroid point based on closest distance.\n",
    "    - use each new group to create a new centroid point which is a mean of all the points in the group\n",
    "    - check to see if the new centroid points are same as the old ones\n",
    "        - If yes, stop the loop and return the centroid points\n",
    "        - else, continue on to the next iteration\n",
    "\n",
    "**NOTE:** \n",
    "1. There is an error in the parameter type as it only incorporates 2 dimensional points, whereas the code is tested on 3 dimensional points as well. Make sure to write a dimension agnostic code.\n",
    "2. I am adding two solution (with/without numpy) for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95e6374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n",
    "    groups=dict()\n",
    "    for _ in range(max_iterations):\n",
    "        new_centroids=[None]*len(initial_centroids)\n",
    "        for i,point in enumerate(points):\n",
    "            group_index=np.argmin([(sum([(x1-x2)**2 for x1,x2 in zip(point,centroid)]))**0.5 for centroid in initial_centroids])\n",
    "            groups[group_index]=groups.get(group_index,[])+[point]\n",
    "        for index,point_list in groups.items():\n",
    "            new_point=[0 for i in range(len(points[0]))]\n",
    "            for point in point_list:\n",
    "                for i in range(len(point)):\n",
    "                    new_point[i]+=point[i]\n",
    "            new_point=[round(val/len(point_list),4) for val in new_point]\n",
    "            new_centroids[index]=new_point\n",
    "        if new_centroids==initial_centroids:\n",
    "            break\n",
    "        initial_centroids=new_centroids\n",
    "    final_centroids=initial_centroids\n",
    "    return final_centroids\n",
    "\n",
    "def k_means_clustering_numpy(points,k,initial_centroids,max_iterations):\n",
    "    points=np.array(points)\n",
    "    centroids=np.array(initial_centroids)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        distances= np.array([np.sqrt((points-centroid)**2).sum(axis=1) for centroid in centroids])\n",
    "        groups=np.argmin(distances,axis=0)\n",
    "        new_centroids= np.array([points[groups==group].mean(axis=0) if len(points[groups==group])>0 else centroids[i] for group in range(k)])\n",
    "        \n",
    "        if np.all(centroids==new_centroids):\n",
    "            break\n",
    "        centroids=new_centroids\n",
    "        centroids=np.round(centroids,4)\n",
    "    return [tuple(centroid) for centroid in centroids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f736761",
   "metadata": {},
   "source": [
    "**Question 4:** Cross-Validation Data Split Implementation\n",
    "\n",
    "Given a dataset, implement a k-fold cross validation split and return the train test split datasets.\n",
    "\n",
    "**Mistakes to remember:**\n",
    "1. np.concatenate takes a list/tuple of arguments. Do not give independent parameters.\n",
    "2. During the kth iteration, make sure to take all the residual samples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a8c0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(data: np.ndarray, k: int) -> list:\n",
    "    fold_size=len(data)//k\n",
    "    folds=[]\n",
    "    for i in range(k):\n",
    "        start=i*fold_size\n",
    "        end=(i+1)*fold_size if i<k-1 else len(data)\n",
    "        test_data=data[start:end].tolist()\n",
    "        train_data=np.concatenate((data[:start],data[end:])).tolist()\n",
    "        folds.append([train_data,test_data])\n",
    "    return folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
